{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89153e1d",
   "metadata": {},
   "source": [
    "### LDA with multiple topic counts (6, 10, 15) forvideos\n",
    "\n",
    "This script tries multiple topic counts (6, 10, 15), saves a bar chart PNG for each count (to outputs/reports/images/), prints top words per topic (so we can label bars in the report), reports LDA perplexity for quick comparison, handles empty/missing descriptions safely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "015b9250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919146d1",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9d718db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODALITY = \"books\"  # \"courses\" | \"books\" | \"videos\"\n",
    "\n",
    "# INPUT_CSV = r\"C:\\Users\\jvlas\\source\\repos\\TrioLearn\\backend\\data\\interim\\videos_metadata_clean.csv\"\n",
    "INPUT_CSV = None\n",
    "\n",
    "# Column with text to model:\n",
    "TEXT_COL = \"description\"       \n",
    "\n",
    "# Topic counts to try:\n",
    "K_LIST = [6, 10, 15]\n",
    "\n",
    "# Cleaned column name\n",
    "CLEAN_COL = \"description_clean\"\n",
    "\n",
    "# How many top words to export per topic\n",
    "TOP_WORDS = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67835da",
   "metadata": {},
   "source": [
    "### Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14cb04b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[check] Repo base: c:\\Users\\jvlas\\source\\repos\\TrioLearn\n",
      "[check] Reading: c:\\Users\\jvlas\\source\\repos\\TrioLearn\\backend\\data\\interim\\books_metadata.csv\n",
      "[check] Images -> c:\\Users\\jvlas\\source\\repos\\TrioLearn\\outputs\\reports\\images\n",
      "[check] CSVs    -> c:\\Users\\jvlas\\source\\repos\\TrioLearn\\outputs\\reports\\csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def resolve_base() -> Path:\n",
    "    \"\"\"Resolve TrioLearn repo root for both .py and notebook runs.\"\"\"\n",
    "    try:\n",
    "        return Path(__file__).resolve().parents[2]\n",
    "    except NameError:\n",
    "        # If run in notebook / interactive\n",
    "        cwd = Path.cwd()\n",
    "        # If already at repo root (TrioLearn), keep it\n",
    "        if (cwd / \"backend\").exists() and (cwd / \"outputs\").exists():\n",
    "            return cwd\n",
    "        # If inside outputs/reports, go up to repo\n",
    "        if cwd.name == \"reports\" and cwd.parent.name == \"outputs\":\n",
    "            return cwd.parents[1]\n",
    "        # If inside outputs, go up one\n",
    "        if cwd.name == \"outputs\":\n",
    "            return cwd.parent\n",
    "        # Fallback: assume current\n",
    "        return cwd\n",
    "\n",
    "BASE = resolve_base()\n",
    "\n",
    "if INPUT_CSV is None:\n",
    "    # Auto paths (change filename here per modality if needed)\n",
    "    default_map = {\n",
    "        \"courses\": BASE / \"backend\" / \"data\" / \"interim\" / \"courses_metadata_clean.csv\",\n",
    "        \"books\":   BASE / \"backend\" / \"data\" / \"interim\" / \"books_metadata.csv\",\n",
    "        \"videos\":  BASE / \"backend\" / \"data\" / \"interim\" / \"videos_metadata_clean.csv\",\n",
    "    }\n",
    "    INPUT_CSV = default_map.get(MODALITY, None)\n",
    "\n",
    "if INPUT_CSV is None:\n",
    "    print(\"[error] Could not resolve INPUT_CSV path. Please set INPUT_CSV explicitly.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "INPUT_CSV = Path(INPUT_CSV)\n",
    "OUT_IMG_DIR = BASE / \"outputs\" / \"reports\" / \"images\"\n",
    "OUT_CSV_DIR = BASE / \"outputs\" / \"reports\" / \"csv\"\n",
    "\n",
    "print(f\"[check] Repo base: {BASE}\")\n",
    "print(f\"[check] Reading: {INPUT_CSV}\")\n",
    "print(f\"[check] Images -> {OUT_IMG_DIR}\")\n",
    "print(f\"[check] CSVs    -> {OUT_CSV_DIR}\")\n",
    "\n",
    "if not INPUT_CSV.exists():\n",
    "    print(f\"[error] CSV not found: {INPUT_CSV}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "OUT_IMG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_CSV_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b91b0b5",
   "metadata": {},
   "source": [
    "### Load input cvs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f660fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[check] Loaded shape: (3507, 15)\n",
      "[check] Columns: ['keyword', 'volume_id', 'title', 'authors', 'description', 'categories', 'publishedDate', 'pageCount', 'language', 'averageRating', 'ratingsCount', 'previewLink'] ...\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(INPUT_CSV)\n",
    "print(f\"[check] Loaded shape: {df.shape}\")\n",
    "print(f\"[check] Columns: {list(df.columns)[:12]} ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d508134",
   "metadata": {},
   "source": [
    "### Pick an ID column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ccc901e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[check] Using ID column: item_id\n"
     ]
    }
   ],
   "source": [
    "def pick_id_col(dframe: pd.DataFrame) -> str:\n",
    "    for c in [\"global_id\", \"course_id\", \"id\", \"external_id\", \"uuid\", \"book_id\", \"video_id\"]:\n",
    "        if c in dframe.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "ID_COL = pick_id_col(df)\n",
    "if ID_COL is None:\n",
    "    df = df.reset_index().rename(columns={\"index\": \"item_id\"})\n",
    "    ID_COL = \"item_id\"\n",
    "print(f\"[check] Using ID column: {ID_COL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19f6d25",
   "metadata": {},
   "source": [
    "### Cleaning: remove URLs & URL-like tockens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99af8bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[check] Cleaning done. Sample:\n",
      "                                         description  \\\n",
      "0  Just like electricity, Machine Learning will r...   \n",
      "1  This comprehensive encyclopedia, in A-Z format...   \n",
      "2  FUNDAMENTALS AND METHODS OF MACHINE AND DEEP L...   \n",
      "\n",
      "                                   description_clean  \n",
      "0  Just like electricity Machine Learning will re...  \n",
      "1  This comprehensive encyclopedia in A-Z format ...  \n",
      "2  FUNDAMENTALS AND METHODS OF MACHINE AND DEEP L...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "URL_TOKEN_PATTERN = re.compile(\n",
    "    r\"\"\"(?ix)\n",
    "    (https?://\\S+)|\n",
    "    \\b(www|youtu|youtube|bit|ly|com|http|https|net|io|amzn|gle|linkedin|twitter|playlist|watch|youtu\\.be)\\b\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "def clean_text_url_tokens(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        s = \"\" if pd.isna(s) else str(s)\n",
    "    s = re.sub(r\"https?://\\S+|www\\.\\S+\", \" \", s)      # remove full URLs\n",
    "    s = URL_TOKEN_PATTERN.sub(\" \", s)                 # remove url-like tokens\n",
    "    s = re.sub(r\"[^A-Za-z0-9\\s\\-\\_\\.]\", \" \", s)       # keep simple chars only\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "if TEXT_COL not in df.columns:\n",
    "    raise KeyError(f\"[error] TEXT_COL='{TEXT_COL}' not in columns of {INPUT_CSV}\")\n",
    "\n",
    "df[CLEAN_COL] = df[TEXT_COL].map(clean_text_url_tokens)\n",
    "print(f\"[check] Cleaning done. Sample:\\n{df[[TEXT_COL, CLEAN_COL]].head(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ed4abb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lda_on_col(dframe: pd.DataFrame, text_col: str, n_topics: int, modality: str, random_state: int = 42):\n",
    "    txt = dframe[text_col].astype(str).fillna(\"\").str.strip()\n",
    "    mask = txt.str.len() > 0\n",
    "    texts = txt[mask]\n",
    "    if texts.empty:\n",
    "        raise ValueError(f\"No non-empty docs in '{text_col}'\")\n",
    "\n",
    "    vec = CountVectorizer(stop_words=\"english\", max_df=0.95, min_df=2)\n",
    "    X = vec.fit_transform(texts)\n",
    "    if X.shape[1] == 0:\n",
    "        vec = CountVectorizer(stop_words=\"english\", max_df=1.0, min_df=1)\n",
    "        X = vec.fit_transform(texts)\n",
    "\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, learning_method=\"batch\", random_state=random_state)\n",
    "    theta = lda.fit_transform(X)\n",
    "    dom = pd.Series(theta.argmax(axis=1), index=texts.index, name=\"dom_topic\")\n",
    "\n",
    "    topic_col = f\"{modality}_dom_topic_K{n_topics}\"\n",
    "    out = dframe.copy()\n",
    "    out[topic_col] = np.nan\n",
    "    out.loc[mask, topic_col] = dom.values\n",
    "    return out, lda, vec, X, topic_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dff0a924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_words_table(lda, vectorizer, topn=TOP_WORDS) -> pd.DataFrame:\n",
    "    terms = np.array(vectorizer.get_feature_names_out())\n",
    "    rows = []\n",
    "    for k, comp in enumerate(lda.components_):\n",
    "        idx = np.argsort(comp)[::-1][:topn]\n",
    "        for rank, j in enumerate(idx, start=1):\n",
    "            rows.append({\"topic\": k, \"rank\": rank, \"word\": terms[j], \"weight\": comp[j]})\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5783a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_label(words: list, max_words=3) -> str:\n",
    "    return \" / \".join(words[:max_words])\n",
    "\n",
    "def join_words(ws, n=10):\n",
    "    return \", \".join(ws[:n])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e434506",
   "metadata": {},
   "source": [
    "### Run script for chosen modality with several K = 6, 10, 15. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4388da37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fitting LDA (MODALITY=books, K=6) ===\n",
      "[metric] Perplexity(K=6): 1,518.14\n",
      "[save] c:\\Users\\jvlas\\source\\repos\\TrioLearn\\outputs\\reports\\csv\\books_dom_topics_K6_CLEAN.csv (3063 labeled rows)\n",
      "[save] c:\\Users\\jvlas\\source\\repos\\TrioLearn\\outputs\\reports\\csv\\books_topics_topwords_K6_CLEAN.csv\n",
      "[save] c:\\Users\\jvlas\\source\\repos\\TrioLearn\\outputs\\reports\\csv\\books_topic_distribution_K6_CLEAN.csv\n",
      "[save] c:\\Users\\jvlas\\source\\repos\\TrioLearn\\outputs\\reports\\images\\books_topic_distribution_K6_CLEAN.png\n",
      "\n",
      "=== Fitting LDA (MODALITY=books, K=10) ===\n",
      "[metric] Perplexity(K=10): 1,506.61\n",
      "[save] c:\\Users\\jvlas\\source\\repos\\TrioLearn\\outputs\\reports\\csv\\books_dom_topics_K10_CLEAN.csv (3063 labeled rows)\n",
      "[save] c:\\Users\\jvlas\\source\\repos\\TrioLearn\\outputs\\reports\\csv\\books_topics_topwords_K10_CLEAN.csv\n",
      "[save] c:\\Users\\jvlas\\source\\repos\\TrioLearn\\outputs\\reports\\csv\\books_topic_distribution_K10_CLEAN.csv\n",
      "[save] c:\\Users\\jvlas\\source\\repos\\TrioLearn\\outputs\\reports\\images\\books_topic_distribution_K10_CLEAN.png\n",
      "\n",
      "=== Fitting LDA (MODALITY=books, K=15) ===\n",
      "[metric] Perplexity(K=15): 1,506.94\n",
      "[save] c:\\Users\\jvlas\\source\\repos\\TrioLearn\\outputs\\reports\\csv\\books_dom_topics_K15_CLEAN.csv (3063 labeled rows)\n",
      "[save] c:\\Users\\jvlas\\source\\repos\\TrioLearn\\outputs\\reports\\csv\\books_topics_topwords_K15_CLEAN.csv\n",
      "[save] c:\\Users\\jvlas\\source\\repos\\TrioLearn\\outputs\\reports\\csv\\books_topic_distribution_K15_CLEAN.csv\n",
      "[save] c:\\Users\\jvlas\\source\\repos\\TrioLearn\\outputs\\reports\\images\\books_topic_distribution_K15_CLEAN.png\n"
     ]
    }
   ],
   "source": [
    "summary = []\n",
    "for K in K_LIST:\n",
    "    print(f\"\\n=== Fitting LDA (MODALITY={MODALITY}, K={K}) ===\")\n",
    "    dfK, ldaK, vecK, XK, topic_col = fit_lda_on_col(df, CLEAN_COL, n_topics=K, modality=MODALITY, random_state=42)\n",
    "\n",
    "    # Perplexity\n",
    "    perp = ldaK.perplexity(XK)\n",
    "    print(f\"[metric] Perplexity(K={K}): {perp:,.2f}\")\n",
    "\n",
    "    # ---- Exports: per-item dominant topic\n",
    "    out_items = pd.DataFrame({\n",
    "        \"item_id\": dfK[ID_COL],\n",
    "        \"dom_topic\": dfK[topic_col].astype(\"Int64\")\n",
    "    })\n",
    "    csv_items = OUT_CSV_DIR / f\"{MODALITY}_dom_topics_K{K}_CLEAN.csv\"\n",
    "    out_items.to_csv(csv_items, index=False)\n",
    "    print(f\"[save] {csv_items} ({len(out_items.dropna())} labeled rows)\")\n",
    "\n",
    "    # ---- Exports: top-words per topic\n",
    "    tw = top_words_table(ldaK, vecK, topn=TOP_WORDS)\n",
    "    # build suggested labels quickly\n",
    "    label_df = (tw.groupby(\"topic\")[\"word\"].apply(list)\n",
    "                  .reset_index()\n",
    "                  .rename(columns={\"word\": \"top_words_list\"}))\n",
    "    label_df[\"top_words\"] = label_df[\"top_words_list\"].apply(lambda ws: join_words(ws, n=min(10, len(ws))))\n",
    "    label_df[\"suggested_label\"] = label_df[\"top_words_list\"].apply(lambda ws: auto_label(ws, max_words=3))\n",
    "    label_df = label_df.drop(columns=[\"top_words_list\"])\n",
    "    csv_topics = OUT_CSV_DIR / f\"{MODALITY}_topics_topwords_K{K}_CLEAN.csv\"\n",
    "    label_df.to_csv(csv_topics, index=False)\n",
    "    print(f\"[save] {csv_topics}\")\n",
    "\n",
    "    # ---- Exports: distribution counts\n",
    "    counts = out_items[\"dom_topic\"].value_counts().sort_index()\n",
    "    dist_csv = OUT_CSV_DIR / f\"{MODALITY}_topic_distribution_K{K}_CLEAN.csv\"\n",
    "    counts.rename_axis(\"topic\").reset_index(name=\"count\").to_csv(dist_csv, index=False)\n",
    "    print(f\"[save] {dist_csv}\")\n",
    "\n",
    "    # ---- Plot: distribution\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    # Ensure all topics 0..K-1 appear even if empty\n",
    "    cat = pd.Series(pd.Categorical(out_items[\"dom_topic\"], categories=range(K)))\n",
    "    cat.value_counts().sort_index().plot(kind=\"bar\")\n",
    "    plt.title(f\"{MODALITY.capitalize()} â€” Distribution of Dominant Topics (K={K}, CLEAN)\")\n",
    "    plt.xlabel(\"Topic\")\n",
    "    plt.ylabel(\"Number of Items\")\n",
    "    plt.tight_layout()\n",
    "    png_path = OUT_IMG_DIR / f\"{MODALITY}_topic_distribution_K{K}_CLEAN.png\"\n",
    "    plt.savefig(png_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"[save] {png_path}\")\n",
    "\n",
    "    summary.append({\"K\": K, \"perplexity\": perp, \"items_csv\": str(csv_items), \"topics_csv\": str(csv_topics), \"dist_csv\": str(dist_csv), \"plot\": str(png_path)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fce0b31",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f7f401b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[summary]\n",
      "  K=6: perp=1518.14 | items=c:\\Users\\jvlas\\source\\repos\\TrioLearn\\outputs\\reports\\csv\\books_dom_topics_K6_CLEAN.csv | topics=c:\\Users\\jvlas\\source\\repos\\TrioLearn\\outputs\\reports\\csv\\books_topics_topwords_K6_CLEAN.csv | dist=c:\\Users\\jvlas\\source\\repos\\TrioLearn\\outputs\\reports\\csv\\books_topic_distribution_K6_CLEAN.csv | plot=c:\\Users\\jvlas\\source\\repos\\TrioLearn\\outputs\\reports\\images\\books_topic_distribution_K6_CLEAN.png\n",
      "  K=10: perp=1506.61 | items=c:\\Users\\jvlas\\source\\repos\\TrioLearn\\outputs\\reports\\csv\\books_dom_topics_K10_CLEAN.csv | topics=c:\\Users\\jvlas\\source\\repos\\TrioLearn\\outputs\\reports\\csv\\books_topics_topwords_K10_CLEAN.csv | dist=c:\\Users\\jvlas\\source\\repos\\TrioLearn\\outputs\\reports\\csv\\books_topic_distribution_K10_CLEAN.csv | plot=c:\\Users\\jvlas\\source\\repos\\TrioLearn\\outputs\\reports\\images\\books_topic_distribution_K10_CLEAN.png\n",
      "  K=15: perp=1506.94 | items=c:\\Users\\jvlas\\source\\repos\\TrioLearn\\outputs\\reports\\csv\\books_dom_topics_K15_CLEAN.csv | topics=c:\\Users\\jvlas\\source\\repos\\TrioLearn\\outputs\\reports\\csv\\books_topics_topwords_K15_CLEAN.csv | dist=c:\\Users\\jvlas\\source\\repos\\TrioLearn\\outputs\\reports\\csv\\books_topic_distribution_K15_CLEAN.csv | plot=c:\\Users\\jvlas\\source\\repos\\TrioLearn\\outputs\\reports\\images\\books_topic_distribution_K15_CLEAN.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n[summary]\")\n",
    "for r in summary:\n",
    "    print(f\"  K={r['K']}: perp={r['perplexity']:.2f} | items={r['items_csv']} | topics={r['topics_csv']} | dist={r['dist_csv']} | plot={r['plot']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1278690",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
