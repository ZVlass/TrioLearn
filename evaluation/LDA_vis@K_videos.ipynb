{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89153e1d",
   "metadata": {},
   "source": [
    "### LDA with multiple topic counts (6, 10, 15) for Videos\n",
    "\n",
    "\n",
    "This script tries multiple topic counts (6, 10, 15), saves a bar chart PNG for each count (to outputs/reports/images/), prints top words per topic (so we can label bars in the report), reports LDA perplexity for quick comparison, handles empty/missing descriptions safely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "015b9250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cb04b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[check] Using: c:\\Users\\jvlas\\source\\repos\\TrioLearn\\backend\\data\\interim\\videos_metadata_clean.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    BASE = Path(__file__).resolve().parents[2]   # when in a .py under outputs/reports/\n",
    "except NameError:\n",
    "    BASE = Path.cwd()\n",
    "    if BASE.name == \"reports\":\n",
    "        BASE = BASE.parents[1]\n",
    "    elif BASE.name == \"outputs\":\n",
    "        BASE = BASE.parent\n",
    "\n",
    "videos_path = BASE / \"backend\" / \"data\" / \"interim\" / \"videos_metadata_clean.csv\"\n",
    "\n",
    "img_dir      = BASE / \"outputs\" / \"reports\" / \"images\"\n",
    "img_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"[check] Using:\", videos_path)\n",
    "\n",
    "df = pd.read_csv(videos_path)\n",
    "\n",
    "TEXT_COL = \"text_for_embeddings\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ed4abb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit_lda_on_masked(df, text_col, n_topics, random_state=42):\n",
    "    txt = df[text_col].astype(str).fillna(\"\").str.strip()\n",
    "    mask = txt.str.len() > 0\n",
    "    texts = txt[mask]\n",
    "    if texts.empty:\n",
    "        raise ValueError(f\"No non-empty docs in '{text_col}'\")\n",
    "\n",
    "    vec = CountVectorizer(stop_words=\"english\", max_df=0.95, min_df=2)\n",
    "    X = vec.fit_transform(texts)\n",
    "    if X.shape[1] == 0:\n",
    "        vec = CountVectorizer(stop_words=\"english\", max_df=1.0, min_df=1)\n",
    "        X = vec.fit_transform(texts)\n",
    "\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=n_topics, learning_method=\"batch\", random_state=random_state\n",
    "    )\n",
    "    theta = lda.fit_transform(X)  # rows align to texts.index\n",
    "    dom = pd.Series(theta.argmax(axis=1), index=texts.index)\n",
    "\n",
    "    # attach \n",
    "    col = f\"dom_topic_{n_topics}\"\n",
    "    out = df.copy()\n",
    "    out[col] = np.nan\n",
    "    out.loc[mask, col] = dom\n",
    "    return out, lda, vec, X, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dff0a924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_words_per_topic(lda, vectorizer, topn=10):\n",
    "    terms = np.array(vectorizer.get_feature_names_out())\n",
    "    tops = []\n",
    "    for k, comp in enumerate(lda.components_):\n",
    "        idx = np.argsort(comp)[::-1][:topn]\n",
    "        words = terms[idx]\n",
    "        tops.append((k, words))\n",
    "    return tops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60ac1f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_distribution_plot(df, topic_col, n_topics, out_png_path, title_prefix=\"Videos\"):\n",
    "    counts = pd.Series(pd.Categorical(df[topic_col], categories=range(n_topics))).value_counts().sort_index()\n",
    "    plt.figure(figsize=(10,5))\n",
    "    counts.plot(kind=\"bar\")\n",
    "    plt.title(f\"Distribution of Dominant Topics ({title_prefix}) â€” K={n_topics}\")\n",
    "    plt.xlabel(\"Topic\")\n",
    "    plt.ylabel(\"Number of Items\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png_path, dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e434506",
   "metadata": {},
   "source": [
    "### Run script for VIDEOS \n",
    "with several K = 6, 10, 15. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f059ffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topic_grid = [6, 10, 15]  \n",
    "results = []\n",
    "\n",
    "for K in topic_grid:\n",
    "    print(f\"\\n=== Fitting LDA with K={K} ===\")\n",
    "    dfK, ldaK, vecK, XK, colK = fit_lda_on_masked(df, TEXT_COL, n_topics=K, random_state=42)\n",
    "\n",
    "    # perplexity (lower is better)\n",
    "    perp = ldaK.perplexity(XK)\n",
    "    print(f\"[metric] Perplexity(K={K}): {perp:,.2f}\")\n",
    "\n",
    "    # top words\n",
    "    print(f\"[topics] Top words per topic (K={K}):\")\n",
    "    for k, words in top_words_per_topic(ldaK, vecK, topn=10):\n",
    "        print(f\"  Topic {k}: {', '.join(words)}\")\n",
    "\n",
    "    # save plot\n",
    "    png_path = img_dir / f\"Video_topic_distribution_K{K}.png\"\n",
    "    save_distribution_plot(dfK, colK, n_topics=K, out_png_path=png_path, title_prefix=\"Courses\")\n",
    "    print(f\"[save] {png_path}\")\n",
    "\n",
    "    results.append({\"K\": K, \"perplexity\": perp, \"png\": str(png_path), \"topic_col\": colK})\n",
    "\n",
    "print(\"\\n[summary]\")\n",
    "for r in results:\n",
    "    print(f\"  K={r['K']}: perplexity={r['perplexity']:.2f} | plot={r['png']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4388da37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fitting LDA with K=6 ===\n",
      "[metric] Perplexity(K=6): 709.14\n",
      "[topics] Top words per topic (K=6):\n",
      "  Topic 0: https, com, learning, www, youtube, neural, ai, machine, deep, networks\n",
      "  Topic 1: data, science, python, learning, course, ai, https, machine, learn, com\n",
      "  Topic 2: https, bit, ly, http, training, data, program, online, learning, masters\n",
      "  Topic 3: learning, 00, 01, machine, 03, 02, 10, 05, 04, regression\n",
      "  Topic 4: https, com, deeplizard, video, learning, learn, instagram, youtube, course, codebasics\n",
      "  Topic 5: https, com, www, youtube, learning, list, watch, data, machine, playlist\n",
      "[save] c:\\Users\\jvlas\\source\\repos\\TrioLearn\\outputs\\reports\\images\\Video_topic_distribution_K6.png\n",
      "\n",
      "=== Fitting LDA with K=10 ===\n",
      "[metric] Perplexity(K=10): 662.15\n",
      "[topics] Top words per topic (K=10):\n",
      "  Topic 0: https, com, www, python, learning, youtu, machine, 00, youtube, amzn\n",
      "  Topic 1: data, science, python, learning, course, https, com, scientist, www, learn\n",
      "  Topic 2: bit, ly, https, http, training, program, online, masters, data, learning\n",
      "  Topic 3: statistics, learning, machine, science, project, computer, year, python, programming, probability\n",
      "  Topic 4: deeplizard, com, https, video, learn, learning, course, youtube, hivemind, deep\n",
      "  Topic 5: https, com, data, net, www, imp, i384100, gle, http, courses\n",
      "  Topic 6: ai, learning, machine, artificial, course, intelligence, https, learn, neural, com\n",
      "  Topic 7: https, com, www, youtube, playlist, list, learning, instagram, video, twitter\n",
      "  Topic 8: learning, deep, neural, 00, machine, networks, https, course, learn, data\n",
      "  Topic 9: https, com, www, youtube, list, ai, watch, data, learning, python\n",
      "[save] c:\\Users\\jvlas\\source\\repos\\TrioLearn\\outputs\\reports\\images\\Video_topic_distribution_K10.png\n",
      "\n",
      "=== Fitting LDA with K=15 ===\n",
      "[metric] Perplexity(K=15): 622.25\n",
      "[topics] Top words per topic (K=15):\n",
      "  Topic 0: https, com, www, learning, youtu, youtube, machine, video, watch, amzn\n",
      "  Topic 1: data, science, https, course, scientist, com, www, learning, career, learn\n",
      "  Topic 2: ly, bit, https, http, training, online, program, masters, data, edureka\n",
      "  Topic 3: learning, machine, science, project, year, beginners, final, computer, learn, coding\n",
      "  Topic 4: deeplizard, com, https, video, learn, learning, course, youtube, hivemind, deep\n",
      "  Topic 5: https, com, data, net, imp, i384100, www, courses, lukeb, learning\n",
      "  Topic 6: neural, ai, networks, network, learning, artificial, intelligence, video, deep, learn\n",
      "  Topic 7: https, com, www, learning, youtube, instagram, linkedin, video, twitter, codebasics\n",
      "  Topic 8: learning, deep, machine, data, course, https, tensorflow, networks, learn, neural\n",
      "  Topic 9: https, www, com, watch, youtube, data, ai, list, learning, ibm\n",
      "  Topic 10: learning, machine, ai, 00, course, https, com, www, 01, youtube\n",
      "  Topic 11: https, com, 00, nan, 01, learning, introduction, 10, 12, deep\n",
      "  Topic 12: python, learn, https, data, course, scikit, learning, programming, machine, pandas\n",
      "  Topic 13: machinelearning, shorts, datascience, coding, ai, stanford, python, programming, ml, artificialintelligence\n",
      "  Topic 14: youtube, https, playlist, list, com, www, io, lecture, engineering, i2dl\n",
      "[save] c:\\Users\\jvlas\\source\\repos\\TrioLearn\\outputs\\reports\\images\\Video_topic_distribution_K15.png\n",
      "\n",
      "[summary]\n",
      "  K=6: perplexity=709.14 | plot=c:\\Users\\jvlas\\source\\repos\\TrioLearn\\outputs\\reports\\images\\Video_topic_distribution_K6.png\n",
      "  K=10: perplexity=662.15 | plot=c:\\Users\\jvlas\\source\\repos\\TrioLearn\\outputs\\reports\\images\\Video_topic_distribution_K10.png\n",
      "  K=15: perplexity=622.25 | plot=c:\\Users\\jvlas\\source\\repos\\TrioLearn\\outputs\\reports\\images\\Video_topic_distribution_K15.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "topic_grid = [6, 10, 15]  \n",
    "results = []\n",
    "\n",
    "for K in topic_grid:\n",
    "    print(f\"\\n=== Fitting LDA with K={K} ===\")\n",
    "    dfK, ldaK, vecK, XK, colK = fit_lda_on_masked(df, TEXT_COL, n_topics=K, random_state=42)\n",
    "\n",
    "    # perplexity (lower is better)\n",
    "    perp = ldaK.perplexity(XK)\n",
    "    print(f\"[metric] Perplexity(K={K}): {perp:,.2f}\")\n",
    "\n",
    "    # top words\n",
    "    print(f\"[topics] Top words per topic (K={K}):\")\n",
    "    for k, words in top_words_per_topic(ldaK, vecK, topn=10):\n",
    "        print(f\"  Topic {k}: {', '.join(words)}\")\n",
    "\n",
    "    # save plot\n",
    "    png_path = img_dir / f\"Video_topic_distribution_K{K}.png\"\n",
    "    save_distribution_plot(dfK, colK, n_topics=K, out_png_path=png_path, title_prefix=\"Videos\")\n",
    "    print(f\"[save] {png_path}\")\n",
    "\n",
    "    results.append({\"K\": K, \"perplexity\": perp, \"png\": str(png_path), \"topic_col\": colK})\n",
    "\n",
    "print(\"\\n[summary]\")\n",
    "for r in results:\n",
    "    print(f\"  K={r['K']}: perplexity={r['perplexity']:.2f} | plot={r['png']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e440c8c",
   "metadata": {},
   "source": [
    "### Run script for videos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7f401b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
