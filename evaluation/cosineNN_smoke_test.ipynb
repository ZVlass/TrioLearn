{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19523a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4267, 5)\n",
      "['modality', 'external_id', 'title', 'topic_vector', 'num_topics']\n",
      "Vector matrix: (4267, 30) | finite: True\n",
      "modality\n",
      "book        2823\n",
      "video        821\n",
      "course       623\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ALL = r\"C:\\Users\\jvlas\\source\\repos\\TrioLearn\\backend\\data\\topics\\all_topic_vectors.parquet\"\n",
    "df = pd.read_parquet(ALL)\n",
    "print(df.shape)\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Try option A: vectors stored in one list/array column\n",
    "vec_col = None\n",
    "for c in df.columns:\n",
    "    s = df[c].dropna().head(10)\n",
    "    if len(s) and s.apply(lambda x: isinstance(x, (list, tuple, np.ndarray))).all():\n",
    "        vec_col = c\n",
    "        break\n",
    "\n",
    "if vec_col is not None:\n",
    "    X = np.vstack(df[vec_col].apply(lambda v: np.asarray(v, dtype=np.float32)).values)\n",
    "else:\n",
    "    # Option B: vectors spread across many numeric columns (e.g., v0..v383)\n",
    "    num = df.select_dtypes(include=[np.number])\n",
    "    if num.shape[1] < 5:\n",
    "        raise ValueError(\"Could not find vector column(s). Inspect df.head().\")\n",
    "    X = num.astype(np.float32).to_numpy()\n",
    "\n",
    "# Normalize for cosine\n",
    "Xn = X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "# Integrity checks\n",
    "assert np.isfinite(Xn).all(), \"Non-finite values in normalized vectors.\"\n",
    "print(\"Vector matrix:\", Xn.shape, \"| finite:\", np.isfinite(Xn).all())\n",
    "print(df[['modality']].value_counts().to_string() if 'modality' in df.columns else \"No modality column\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834029d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prep: ensure df[\"text_for_embed\"] exists and is non-empty ---\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "if \"text_for_embed\" not in df.columns:\n",
    "    # Try to build from any reasonable text columns found in this df\n",
    "    TEXT_CAND = [c for c in [\n",
    "        \"title\",\"course_title\",\"book_title\",\"video_title\",\"name\",\n",
    "        \"description\",\"skills\",\"tags\",\"summary\",\"text\"\n",
    "    ] if c in df.columns]\n",
    "\n",
    "    if not TEXT_CAND:\n",
    "        raise ValueError(\n",
    "            \"No text columns found to build 'text_for_embed'. \"\n",
    "            \"It looks like you loaded a vectors-only table (e.g., all_topic_vectors.parquet). \"\n",
    "            \"Load a metadata CSV (courses/books/videos) that has titles/descriptions, \"\n",
    "            \"or switch to the topic-vector retrieval path.\"\n",
    "            f\"\\nAvailable columns in current df: {list(df.columns)}\"\n",
    "        )\n",
    "\n",
    "    # Coerce to pandas StringDtype and build text_for_embed\n",
    "    for c in TEXT_CAND:\n",
    "        df[c] = df[c].astype(\"string\")\n",
    "\n",
    "    def clean_join(row):\n",
    "        parts = []\n",
    "        for c in TEXT_CAND:\n",
    "            v = row[c]\n",
    "            if v is not pd.NA and v is not None:\n",
    "                s = str(v).strip()\n",
    "                if s:\n",
    "                    parts.append(s)\n",
    "        return \" \".join(parts)\n",
    "\n",
    "    df[\"text_for_embed\"] = df.apply(clean_join, axis=1)\n",
    "\n",
    "# Drop empty/very short texts\n",
    "df = df[df[\"text_for_embed\"].apply(lambda x: isinstance(x, str) and len(x) >= 5)].reset_index(drop=True)\n",
    "print(\"Docs ready for SBERT:\", len(df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f42e7b",
   "metadata": {},
   "source": [
    "# Recommender (SBERT) + smoke test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8635774",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "description",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# --- 0) Coerce candidate text columns to safe string dtype ---\u001b[39;00m\n\u001b[0;32m      6\u001b[0m TEXT_CAND \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskills\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m TEXT_CAND, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m TEXT_CAND:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Pandas StringDtype handles NA cleanly\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     df[c] \u001b[38;5;241m=\u001b[39m df[c]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: description"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# --- 0) Coerce candidate text columns to safe string dtype ---\n",
    "TEXT_CAND = [c for c in [\"title\",\"description\",\"skills\",\"tags\",\"summary\",\"text\"] if c in df.columns]\n",
    "assert TEXT_CAND, \"description\"\n",
    "\n",
    "for c in TEXT_CAND:\n",
    "    # Pandas StringDtype handles NA cleanly\n",
    "    df[c] = df[c].astype(\"string\")\n",
    "\n",
    "# --- 1) Build robust text_for_embed without using .str on the whole Series ---\n",
    "def clean_join(row):\n",
    "    parts = []\n",
    "    for c in TEXT_CAND:\n",
    "        v = row[c]\n",
    "        if v is not pd.NA and v is not None:\n",
    "            s = str(v).strip()\n",
    "            if s:\n",
    "                parts.append(s)\n",
    "    return \" \".join(parts)\n",
    "\n",
    "df[\"text_for_embed\"] = df.apply(clean_join, axis=1)\n",
    "\n",
    "# Drop empty/very short docs (no .str; use len() on Python strings)\n",
    "df = df[df[\"text_for_embed\"].apply(lambda x: isinstance(x, str) and len(x) >= 5)].reset_index(drop=True)\n",
    "print(\"Kept docs:\", len(df), \"| dropped empties/very short texts.\")\n",
    "\n",
    "# --- 2) Embed corpus with SBERT ---\n",
    "_sbert = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "Xn = _sbert.encode(df[\"text_for_embed\"].tolist(), normalize_embeddings=True)\n",
    "Xn = np.asarray(Xn, dtype=np.float32)\n",
    "print(\"Corpus SBERT shape:\", Xn.shape)\n",
    "\n",
    "\n",
    "# --- 3) Safe top-k from query text ---\n",
    "def topk_from_query_text(query_text, Xn, model=_sbert, k=10):\n",
    "    q = (query_text or \"\").strip()\n",
    "    if len(q) < 3:\n",
    "        raise ValueError(f\"Query too short/empty: {repr(query_text)}\")\n",
    "    qv = model.encode([q], normalize_embeddings=True)[0].astype(np.float32)\n",
    "    sims = Xn @ qv\n",
    "    k = min(k, len(sims))\n",
    "    idx = np.argpartition(-sims, k-1)[:k]\n",
    "    idx = idx[np.argsort(-sims[idx])]\n",
    "    return idx, sims[idx]\n",
    "\n",
    "# --- 4) Build a non-empty query pool WITHOUT .str accessor ---\n",
    "TITLE_CANDS = [c for c in [\"title\",\"name\",\"course_title\",\"book_title\",\"video_title\"] if c in df.columns]\n",
    "title_col = TITLE_CANDS[0] if TITLE_CANDS else None\n",
    "\n",
    "if title_col:\n",
    "    mask = df[title_col].apply(lambda x: isinstance(x, str) and len(x.strip()) >= 3)\n",
    "    qpool = df.loc[mask, title_col].tolist()\n",
    "else:\n",
    "    qpool = df[\"text_for_embed\"].tolist()\n",
    "\n",
    "if not qpool:\n",
    "    qpool = [\"deep learning\",\"bayesian inference\",\"computer vision\",\"SQL databases\",\"transformers for NLP\"]\n",
    "\n",
    "queries = pd.Series(qpool).sample(min(5, len(qpool)), random_state=42).tolist()\n",
    "\n",
    "# --- 5) Smoke test ---\n",
    "for q in queries:\n",
    "    idx, sims = topk_from_query_text(q, Xn, k=10)\n",
    "    print(\"\\nQuery:\", q[:80])\n",
    "    for r, (i, s) in enumerate(zip(idx, sims), 1):\n",
    "        title_disp = \"\"\n",
    "        if title_col:\n",
    "            val = df.iloc[i][title_col]\n",
    "            title_disp = (val if isinstance(val, str) else str(val))[:70]\n",
    "        mod = df.iloc[i][\"modality\"] if \"modality\" in df.columns else \"?\"\n",
    "        print(f\"{r:2d}. [{mod}] {title_disp} | cos={s:.3f}\")\n",
    "\n",
    "# --- 6) Sanity: embeddings not collapsed ---\n",
    "print(\"\\nStd of corpus embeddings (should be > 0):\", f\"{float(np.std(Xn)):.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb900525",
   "metadata": {},
   "source": [
    "# Self-retrieval (Recall@1) sanity\n",
    "\n",
    "Pick random items and check whether each item retrieves itself (or a near-duplicate) at rank 1 when using its own text as the query. This catches indexing or alignment bugs. \n",
    "Pass criteria: high Recall@1 (often >0.8) indicates vectors and indexing are consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450c49f2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1faa69aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 384 is different from 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m hits \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, (i, qv) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(sample_idx, Q)):\n\u001b[1;32m---> 18\u001b[0m     idx, sims \u001b[38;5;241m=\u001b[39m \u001b[43mtopk_from_query_vec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# consider “self” if exact index OR very high cosine (>=0.99)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m idx[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m i \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(sims[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.99\u001b[39m:\n",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m, in \u001b[0;36mtopk_from_query_vec\u001b[1;34m(qv, Xn, k)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtopk_from_query_vec\u001b[39m(qv, Xn, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     sims \u001b[38;5;241m=\u001b[39m (\u001b[43mXn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqv\u001b[49m)        \u001b[38;5;66;03m# cosine since rows are normalized\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     idx  \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margpartition(\u001b[38;5;241m-\u001b[39msims, \u001b[38;5;28mmin\u001b[39m(k, \u001b[38;5;28mlen\u001b[39m(sims)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))[:k]\n\u001b[0;32m      4\u001b[0m     idx  \u001b[38;5;241m=\u001b[39m idx[np\u001b[38;5;241m.\u001b[39margsort(\u001b[38;5;241m-\u001b[39msims[idx])]\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 384 is different from 30)"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "sample_idx = rng.choice(len(df), size=min(100, len(df)), replace=False)\n",
    "\n",
    "# Build queries from existing items\n",
    "texts_for_queries = []\n",
    "for i in sample_idx:\n",
    "    if title_col:\n",
    "        texts_for_queries.append(str(df[title_col].iloc[i]))\n",
    "    else:\n",
    "        # fallback: if you have a 'text' or 'description' column\n",
    "        txt_col = next((c for c in [\"text\",\"description\",\"summary\"] if c in df.columns), None)\n",
    "        texts_for_queries.append(str(df[txt_col].iloc[i]) if txt_col else f\"doc_{i}\")\n",
    "\n",
    "Q = embed_text_minilm(texts_for_queries)\n",
    "\n",
    "hits = 0\n",
    "for j, (i, qv) in enumerate(zip(sample_idx, Q)):\n",
    "    idx, sims = topk_from_query_vec(qv, Xn, k=1)\n",
    "    # consider “self” if exact index OR very high cosine (>=0.99)\n",
    "    if idx[0] == i or float(sims[0]) >= 0.99:\n",
    "        hits += 1\n",
    "\n",
    "recall1 = hits / len(sample_idx)\n",
    "print(f\"Self-retrieval Recall@1 on n={len(sample_idx)}: {recall1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52126820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs ready for SBERT: 0\n"
     ]
    }
   ],
   "source": [
    "# Build text_for_embed from whatever columns exist\n",
    "TEXT_CAND = [c for c in [\n",
    "    \"title\",\"course_title\",\"book_title\",\"video_title\",\"name\",\n",
    "    \"description\",\"skills\",\"tags\",\"summary\",\"text\"\n",
    "] if c in df.columns]\n",
    "if not TEXT_CAND:\n",
    "    raise ValueError(f\"No text columns found. Available: {list(df.columns)}\")\n",
    "\n",
    "for c in TEXT_CAND:\n",
    "    df[c] = df[c].astype(\"string\")\n",
    "\n",
    "def clean_join(row):\n",
    "    parts = []\n",
    "    for c in TEXT_CAND:\n",
    "        v = row[c]\n",
    "        if v is not pd.NA and v is not None:\n",
    "            s = str(v).strip()\n",
    "            if s:\n",
    "                parts.append(s)\n",
    "    return \" \".join(parts)\n",
    "\n",
    "df[\"text_for_embed\"] = df.apply(clean_join, axis=1)\n",
    "df = df[df[\"text_for_embed\"].apply(lambda x: isinstance(x,str) and len(x) >= 5)].reset_index(drop=True)\n",
    "print(\"Docs ready for SBERT:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1a8019",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
